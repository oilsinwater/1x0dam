Using Targeted Online Advertising to Counter Disinformation in Germany (Case Study)


Executive Summary
A wide range of ideas and proposals have been put forward by policymakers and platforms alike regarding how best to counter online disinformation, yet few to date have explored the possibility of actively mitigating the problem by engaging with online advertising and data tracking. But if targeted advertising and surveillance capitalism writ large are fuelling the ongoing democratic information crisis, might those same tools – in the right hands – be harnessed to help stem the tide of disinformation? Project Mayday tested this possibility by purchasing targeted ad buys on disinformation websites and related keyword searches in the lead-up to the contested 2021 German federal election with the goal of providing accuracy-nudge interventions to individuals consuming, or about to consume, fake news.

Our findings reveal that surveillance advertising can be used in an efficient manner to reach at-risk disinformation users and apply accuracy-nudge interventions. By purchasing the ability to speak to disinformation’s target group of consumers where and when it is most effective, our project was able to successfully interrupt the consumption of problematic content and prime members of the target audience to more critically view content and assess veracity. Crucially, project data also shows these users will engage with banner ads asking them to identify fake news – demonstrating a desire to participate in an additional opportunity to learn more via a content-neutral external website. These findings are particularly significant in the global fight to curb the impact of disinformation given that existing research has shown how content-neutral, accuracy-nudge interventions like these refocus user priorities on the value of accuracy, induce better sharing habits, and increase user ability to discern what is true and what is false. 

Because neither the state nor the market is sufficiently responding to the threat of disinformation online, there remains an urgent need going forward for members of civil society to step in and help address this issue. Our case study therefore invites other organizations and initiatives to conduct similar projects, providing a roadmap on project design and implementation, and offering insights into ways by which future efficacy and reach can be increased by measuring engagement impact and adding user journey granularity. In the meantime, the following policy implications attach to our current findings:

Targeted public awareness campaigns, modeled after the research conducted in this project, should be employed by governments and foundations around the world as a new tool in their disinformation intervention toolboxes. 
Policymakers and companies should work to revamp the digital advertising ecosystem in a large-scale effort to disincentivize disinformation.
When evaluating the type of lasting impact created by educating individuals via targeted ad banner interventions, increased efforts should also be made to institutionalize digital literacy as a more comprehensive response to the threat posed by disinformation.
About Us
Inspired by the Mayday PAC (the “super PAC to end all super PACs”), Project Mayday is a collection of technologists, activists, researchers, and designers using the tools of surveillance capitalism to conduct experiments that help expose harms, spur policy reform, and speed the end of this extractive, exploitative, and dehumanizing system.
Introduction
Disinformation is a common problem across the globe, one made worse by targeted advertising. Yet while advertising and surveillance capitalism writ large fuel this problem, might those same tools be used to work against disinformation? Our team set out to test this possibility by purchasing targeted ad buys on disinformation websites and related keyword searches – with the goal of using these ads to prime readers for accuracy when viewing disinformation in the lead-up to the contested 2021 German federal election.

Our findings discussed in this report demonstrate that banner ads can be used in this manner to reach at-risk disinformation users, and these users will in fact engage with these ads in this way. These findings are significant in the global efforts to curb the spread and impact of disinformation because existing research on the benefits of priming for accuracy indicates the type of interactions generated by the ad buys should have a positive impact on these users’ relationship with disinformation. Not only will the ads interrupt their consumption of the problematic content, the banners will also increase user awareness and desire to view information more critically – while offering an additional opportunity to learn more via a content-neutral external website. Accuracy-nudge interventions such as these are proven to result in a reduced desire to share problematic content and an increased awareness/ability to discern content veracity.

Although more research needs to be done to confirm these findings and assess to what extent engaging with this type of content changes a reader’s relationship with disinformation, important policy implications attach to our current findings – most notably that narrowly focused public awareness campaigns would be highly effective in reaching at-risk consumers of disinformation.
Project Overview
Background
As the online sphere continues to evolve, anti-democratic regimes have begun to expand their tactics of suppression, censorship, and surveillance. Instead of working solely to constrict the flow of information, these regimes now seek to advance their aims by also flooding the online space with unprecedented amounts of digitally powered disinformation designed to polarize democratic societies by stoking fear and hatred. In order to protect the fundamental principles of democracy, the current information environment must be reimagined for the digital age. 

Unfortunately, while Big Tech corporations such as Facebook are increasingly implicated as breeding grounds for the spread of disinformation, these companies have shown little willingness to self-regulate their platforms in a manner that prioritizes the public good – and in turn lobby diligently against any proposed government oversight. As a result, while ongoing and insistent calls for government regulation of online platforms exist, progress in implementing effective measures is all too often stymied or watered down. The need for independent third-parties to develop new and innovative interventions is therefore pressing.
Purpose + Hypotheses
Given that neither the state nor the market is sufficiently responding to the threat of disinformation online, there remains an urgent need for civil society to step in and attempt to address the issue. A wide range of ideas and proposals have been put forward on the matter, but few have explored the possibilities of mitigating disinformation by actively engaging with one of the core structures of surveillance capitalism: online advertising and data tracking. The purpose of this project was to change that by purchasing the ability to speak to disinformation’s target group of consumers where and when it is most effective. In short, our team wondered what would happen if paid online advertisements on disinformation websites and related keyword searches were purchased and filled with banners asking readers to identify fake news. Could the machine of surveillance capitalism be used for good?

The idea of working to reach disinformation consumers when and where it matters most relied on scientifically proven accuracy-nudge techniques. As explained below, lab research has shown that when people consuming misinformation are asked if they can identify such content, their ability to do so significantly increases and their willingness to share dramatically decreases. Theoretically, therefore, if readers engage with advertisements asking them to identify fake news, this action should have a notable positive impact on their behavior. And by simply being exposed to the question and subject matter in the first place, a reader is also automatically primed for accuracy – regardless of whether or not they further engage by clicking the advertisement. In addition to focusing readers’ attention on the critical issue of accuracy, such ads would give them the opportunity to visit a site that offers quizzes and other opportunities for further education on disinformation. Whenever the ads were clicked on, that data point would also create an algorithmic impact demonstrating that people reading the content on the site might prefer something different (which in turn would affect what they are likely to see in the future).

Despite these promising possibilities, however, it was unknown whether targeted advertising could in fact be used to reach these at-risk disinformation users, and whether members of the target audience would actually engage with such advertisements when presented with them. This research project was therefore designed to test out two key hypotheses:

Hypothesis A: targeted advertising can be used to reach people who are consuming, or are about to consume, disinformation on websites.
Hypothesis B: such people are likely to engage with banner ads that ask them to identify disinformation.

As discussed below, if these hypotheses proved to be correct, such exposure and engagement would likely create notable behavioral benefits such as reduced time spent on disinformation sites, reduced sharing, and a reduced likelihood of return. Still, before our team could test whether engaging with this kind of content actually does create such behavioral benefits (an inquiry ultimately better suited for future studies), we first needed to establish whether ads could be used in an effective manner to target people consuming disinformation, and whether those individuals would subsequently engage with these types of banner ads.
Prior Related Work
A similar pilot project was conducted by certain members of our research team during the 2020 U.S. election. As part of the pilot, advertising banners were placed on websites like https://www.breitbart.com. Once clicked, these ads linked users to https://www.spotfakenews.info – a site offering quizzes about fake news and providing articles with tips for how to spot this type of content. Although the effort indicated a likelihood that the hypothesis for engagement would be validated with positive behavioral impacts, no results were published due to the preliminary nature of the pilot study.

 
Screenshots from U.S. ad placements and www.spotfakenews.info.

In an effort to refine this work and produce a public-facing report to spur additional inquiries and policy reform, Project Mayday sought to once again test this approach in the leadup to the 2021 German federal election (in which a significant disinformation campaign was anticipated). This time, instead of focusing on U.S.-based websites, the project would identify sites linked to German disinformation networks on which banner ads could be placed with the goal of interrupting user behavior, emphasizing accuracy, and educating users about fake news. 
Literature Review + Application
Existing Research
The core concept underlying this project was to purchase paid online advertisements on disinformation websites and related keyword searches, using the ads to ask people to identify fake news. The rationale behind this form of intervention stems from research conducted by Gordon Pennycook, et al. (hereinafter “Pennycook team”), on the matter of why people share misinformation and whether certain simple interventions could be used to help mitigate the problem. Primary research conducted by the Pennycook team previously established that people in general have a strong preference to only share true content on social media. Despite this preference, however, false content is routinely shared and spread online. The Pennycook team examined this apparent paradox first in a lab, and then “in the field” through an experiment on Twitter, and found the discrepancy is likely a result of a lack of attention being paid to content accuracy prior to sharing.

Based on its research, the Pennycook team advanced the “inattention-based account”:

in which (i) people do care more about accuracy than other content dimensions, but accuracy nonetheless often has little effect on sharing, because (ii) the social media context focuses their attention on other factors such as the desire to attract and please followers/friends or to signal one’s group membership.

In sum, people really would like to share only content that is true – but when they are on social media, this preference falls to the wayside because their attention is focused on other factors. Critically, the Pennycook team’s findings directly challenge the common assumption that people value partisanship over accuracy (a stereotype often used to explain the sharing of misinformation online), and indicate that content-neutral interventions highlighting the concept of accuracy could be used to help mitigate the spread and impact of mis- and disinformation online.

Applying the inattention-based account to lab studies, the Pennycook team found that “shifting attention to accuracy” resulted in participants being “significantly less likely to consider sharing false headlines.” To establish this connection, participants in the treatment condition were asked at the start of the experiment to “rate the accuracy of a single non-partisan news headline.” When compared to the results of the control group, this simple act of priming for accuracy resulted in a significantly improved desire to share accurate content. After establishing this lab-based foundation, the Pennycook team conducted a “digital field experiment on social media” to determine whether the lab results would apply and hold true in a real world setting. To do so, they sent direct messages to Twitter users who had previously shared links to websites that independent fact-checkers rated as “highly untrustworthy.” Similar to the accuracy prompt in the lab-based treatment condition, the messages merely asked users to rate the accuracy of a single non-political headline. After sending the message, the Pennycook team observed the user’s sharing activity over the subsequent 24 hours – once again finding the same type of positive impact by which users become more “discerning in their subsequent sharing decisions.”

An additional study by Pennycook and related researchers determined these findings on “accuracy-nudge interventions” apply not only in the context of political fake news, but also to misinformation related to the COVID-19 pandemic. Once again, lab-based research demonstrated that “nudging people to think about accuracy is a simple way to improve choices about what to share on social media.” Priming users to think about accuracy in a content-neutral way consistently proved to be an effective way to refocus user priorities and mitigate the myriad distractions that otherwise prevent users from considering content veracity prior to sharing.

Notably, findings from the COVID-19 study also suggest that partisanship is not the key factor distracting people and inducing them to share false content. The researchers in that study surmised that distraction from accuracy on social media “seems more general” and may indeed be a result (intended or not) of platform design. Platforms “provide immediate, quantified feedback” and as a result, users may gloss over accuracy to focus instead “on other factors, such as concerns about social validation and reinforcement.” The fact that news is also mixed in with content in which accuracy is not relevant or applicable, such as pictures of food or cute babies, could also contribute to this heightened state of distraction and inability to focus on accuracy (which otherwise would be a priority).
Application of Accuracy-Nudge Interventions
The above-discussed existing research provided a strong framework off which our team could apply accuracy-nudge interventions to real world disinformation consumers. The findings established by Pennycook’s team demonstrate that accuracy as a value is important for users when sharing content – but when on social media, these users are often distracted from considering the accuracy of the content they are viewing. As a result, shifting the focus and prompting thoughts of accuracy can produce demonstrably better sharing habits. Once initially primed in a content-neutral way to think about whether or not a headline or piece of information is true, user ability to discern what is true and what is false increases.

In assessing the various levels and degrees of user distraction, members from our project team also wondered whether a latent historical bias might be at play in affecting user ability to prioritize accuracy when assessing online content. The majority of offline content consumed by users through sources such as school, books, newspapers, and traditional news broadcasting require little to no attention to accuracy when being consumed. Instead, the default (and indeed reasonable) assumption is that such content has already been pre-vetted and therefore is accurate. The advent of unregulated internet content – spread through blogs, social media, and disinformation sites – has fundamentally changed this calculus. Yet not all users are necessarily aware of this shift – and even those who are could still benefit from an accuracy-nudge intervention reminding them that online content should be viewed with a discerning eye, not automatically taken at face value. By simply asking “Can you spot fake news?”, online users will be primed to activate a portion of their thought-processes which otherwise understandably lie dormant during offline content consumption.

Though the COVID-19 Pennycook research team suggested a “suite” of interventions that social media platforms could easily apply to help mitigate the spread of misinformation, these companies have so far failed to implement them in a meaningful (if any) way. Given this lack of action, and the compounding misinformation problem exemplified by the online incitement of the storming of the U.S. Capitol, our team decided to step in and test an additional type of accuracy-nudge intervention utilizing the mechanisms of surveillance capitalism. By purchasing targeted ad buys on disinformation websites and related keyword searches, we would be able to prime users for accuracy in a space where lots of disinformation initially originates. In addition, users on such sites are highly likely to be in the process of consuming disinformation – meaning such an intervention would speak directly to those that matter most when it matters most. Notably, this type of targeted advertising can also reach users outside of the social media context, by appearing in search engine results tied to specific keywords, offering a unique digital counteradvertising tool to the list of available interventions.

In addition, the Pennycook team’s research indicates that content-neutral approaches to priming for accuracy are effective – eliminating the need to rely “on a centralized institution to certify truth and censor falsehood.” Instead, simply putting the concept of fake news on a user’s radar is a sufficient reminder to activate a state of higher discernment when consuming content, allowing them to act in line with their desire to share only accurate information. Application of such a framework is particularly useful in the context of disinformation websites, where users might be skeptical of a third-party source telling them what is or is not true. A simple nudge towards accuracy is therefore not only sufficient – but likely may be more effective for our target users.
Ethical Considerations
Throughout the course of the project, our team was forced to grapple with various ethical considerations regarding whether the ends justified the means of the research processes. Three prominent considerations rose to the forefront. First, is it legitimate to fight surveillance capitalism by using its own means? On principle, members of the team oppose the systemic privacy invasions, harms, and warped societal norms produced by the de facto business model of today’s internet. Yet the overarching goal behind Project Mayday is to expose the system so that it can be changed – and most casual internet users are unaware of the ways the surveillance capitalism uses their data for profit. Part of exposing the system, then, is experimenting with its practices with the express intent of publishing precisely how those processes work and exposing their direct impacts on everyday users. In short, our team had to experiment with the machine in order to collect the evidence necessary to both expose and ultimately help change the system. Nonetheless, this reality did not give us carte blanche to engage in any targeted advertising practice whatsoever. Rather, our experimentation with the machine would necessarily occur on a slippery slope that would need to be consistently monitored. This meant that the content of our experimentation would need to be beneficial to users in general. 

Accordingly, at each step of the way, we worked to examine our decisions in an effort to justify them. Not only did we need to consider our perception that doing the experiment at all was net positive for the movement against surveillance capitalism; the content of our ad campaigns as an intervention against the spread of disinformation was just as critical. If we could improve the net impacts of our ads and campaigns by tracking the data of user journeys, was doing so justified? If so, to what degree? No easy or clear cut answers presented themselves, yet in the end our team understood that a certain degree of experimentation was necessary given that the ultimate end – an exposé of surveillance capitalism to help spur lasting reform – was so important. And in the more immediate near-term, the end of establishing a new means by which to help stem the spread of disinformation (this time through targeted ads) was highly significant as well.

The second ethical consideration that posed a challenge to the project was the fact that placing banners on disinformation websites would require ads to be purchased on the websites – a problematic move in and of itself. Providing some degree of revenue, as well as increased advertising standing, to websites working to distort truth is not a desirable option. Yet again, in this case, we deemed the ends necessary to justify the means because these purchases would allow our team to provide content that would potentially help break the cycle of disinformation and stem its negative impacts. This process also helped us to understand how platforms regulate (or fail to regulate) ad content, further informing our ability to expose the machinations of the system. 

The final core ethical consideration we engaged with was whether it was acceptable to, in an effort to avoid alienating members of our target audience, imitate the style and aesthetics of typical disinformation content. Was it legitimate to deploy polarizing language and aesthetics in order to draw people’s attention and ultimately educate them about fake news? This debate occurred at all levels of the project: from the banner design to the website content, and extended out to the question of whether we should include an impressum on www.echtwahr.info. The consideration generally involved balancing integrity with effectiveness. The team did not want to entirely leave out an impressum because doing so would mean replicating the lack of sources that is characteristic of disinformation. At the same time, however, including a full impressum could risk alienating certain members of the target audience (causing them to leave the website) or make the owners of the disinformation sites themselves potentially work to have the banners removed and prevent their placement in the future. 

At the same time, avoiding the use of a full impressum also risked discrediting the initiative (at least at first blush) in the eyes of potential allies, such as experts and journalists focused on German disinformation. For example, one journalist tweeted this out after she came across the website:


Screenshots of the journalist’s tweet. English translation: “In principle, I don't think it's bad in these times to have an initiative that helps to identify false reports. But I would like to know who is behind it and I can't find an imprint?!”

In the end, this type of response (and its subsequent comments) served as validation that the campaign’s goals were being achieved because it signaled that people researching and reading in the project’s target space were seeing and engaging with the ads. At first, the journalist could not ascertain the purpose or source of the banners. After engaging with the content, however, she and her allies ultimately concluded that the information would be very helpful for the audiences who typically go to the disinformation websites at issues. In their eyes, the website was simple – but professional – and looked quite different from stereotypical progressive efforts (a factor we considered to be an asset in this case). 
UX Design + Implementation
Localizing for a German Audience 
During the first phase of the project, the same tools that were used in the pilot project to map U.S. disinformation networks were applied to Germany in order to identify the linked sites, audience flow, and engagement rates. Ultimately, ads were purchased on websites the team regarded as significant disinformation spreaders in the German election, including Russia Today Germany (https://de.rt.com) and The Epoch Times (https://www.epochtimes.de). In turn, banners would be placed on those sites that would link to www.echtwahr.info (a German equivalent of www.spotfakenews.info, meaning “really true”). Similar to the pilot project run in the U.S., this site would contain information to educate readers about fake news and offer an opportunity to take quizzes to see if a reader could indeed “spot” false content. 

The top sites targeted in the ad-buying campaign were: 

The Epoch Times
RT DE
ANTI-SPIEGEL
Reitschuster
ScienceFiles
JournalistenWatch
Corona Ausschuss
Sputnik
FreieWelt
Rubikon
Kopp Report
Kopp Verlag
Die Unbestechlichen
Deutschland Kurier
SNA News

Prior to purchasing the ads, the team worked to properly contextualize the U.S. pilot program’s framework so that it would resonate on a cultural level in Germany. A direct 1:1 translation of the original UX design would potentially risk alienating members of the target audience, as it would be unreasonable to assume that the visual identity, slogans, and user journey that had worked in the U.S. would automatically resonate with German consumers of fake news. 

To ensure a proper cultural fit for the German campaign, the Mayday team designed new UX experiences and slogans for advertising banners, as well as an updated structure for the www.echtwahr.info website. Although the primary objective of the project was to see if banner ads would be a viable means by which to reach people on disinformation sites, the UX design team also worked to create an optimal environment for the accuracy-nudge effect to occur after engaging with an ad and being redirected to the website. As a result, user behavior goals included reduced time spent on disinformation sites, reduced sharing of false posts, and a reduced likelihood of return to the original disinformation website. Whatever user data was generated by these engagements (if ultimately collected) could also potentially be used to help demonstrate how disinformation operates – emphasizing how easy it is to disseminate politically-motivated content on the internet.
Audience Analysis
In order to ascertain the specific type of verbal and visual language that would best facilitate the desired user behavior, the UX team analyzed the likely preferences and identities of the German campaign’s target audience. Due to differences in culture and context, many of these were not the same as those involved in the original U.S. campaign.

The visual language and slogans utilized during the U.S. pilot program centered on making users feel as “normal” as possible, with slogans suggesting that they should spot fake news because “most” Americans would. The pilot also strategically deployed Russophobic references due to their resonance in conservative U.S. circles (by contrast, the German right-wing party Alternative for Germany, AfD, is known for its pro-Russian stance). Unlike in the U.S. campaign, the team needed to determine whether the invocation of the language and aesthetics of the far right should be avoided given the increased sensitivity of the German NGO and journalistic communities (which could potentially discredit the campaign). 

An expert in German fake news and audiences was therefore brought on board to expand this work. The assumption the expert started from was that the intended audience could best be analyzed by looking at the way they interacted with each other and shared disinformation stories on other platforms – in particular Telegram, which has become a primary meeting point for the far right and conspiracy milieu. 

The expert joined and analyzed Telegram groups in an effort to establish the overarching narratives, thematic interests, and debates among the intended audience. The focus was to understand the tone of voice that was appealing and “natural” within the community, as well as themes that piqued interest and elicited engagement. A similar analysis was also conducted on Facebook groups.


Screenshots of some of the large groups joined by the expert in conducting audience analysis.

Analysis of the topics discussed in these groups revealed a series of core themes that seemed to particularly resonate with the target audience, including climate change, Corona, antisemitism, and antifeminism. 


Mood boards grouped by theme were used to depict language and visuals.

From this investigation, an overarching narrative emerged in which members of the target audience position themselves as “dissidents” to a dominant discourse perpetuated by Western governments and the mainstream media. Interestingly, members of the analyzed groups appeared to be very concerned with “disinformation” and “fake news” supposedly spread by the mainstream. 


Screenshot of a common narrative expressed by members of the analyzed groups.

Ultimately, conversations in the groups seemed to suggest that the members knew something that the government was disguising. In turn, a strong sense of community seemed to derive from that insider knowledge, implying a shared sense of chosenness. These takeaways, and other related research and audience analysis, were subsequently applied to the design implementation of the project.
Design Implementation
Based on the above research, guiding principles were established to inform content creation. Given that members of the project’s target group appeared to position themselves as “dissidents” to a dominant narrative, it seemed that anything that might appear “patronizing” would be off-putting and risk inducing an undesired response. Prior studies also confirm that a “patronizing” mode of communicating with conspiracy believers can foster a so-called “nasty mechanism” whereby the individual is provoked by the “educating” statement and begins to aggressively contradict that which is being offered to educate. Because studies also show that even content that is only briefly consumed on the internet can shape cognition in the long term, our team determined we would not design anything polarizing or racist (even in an attempt to increase engagement).

Accordingly, the following three principles were formulated to help guide the design process:

Project language should not antagonize.
Project language should make readers feel as if they are partaking in something larger (speaking to a sense of pride and chosenness).
Project language should not enhance conspiratorial or right-wing thinking.

Each of these three principles informed the creation of ad slogans, website structure, and copy content.
Banner Design
The goal in creating banner content and design was to find an issue that could suggest participation, or ask people’s opinion, without perpetuating any sort of racist or otherwise problematic disposition. Interestingly, the issue ultimately identified as a potential “uniting cause” was the fight against “fake news” itself. In turn, banners were designed to impart the feeling that if viewers clicked on a banner, they would be directed to a website where they could contribute to the fight against fake news. 

The regional expert emphasized the degree to which talk of “fake news” and the self-declared mission to contest is prevalent on Russia Today and other disinformation websites in Germany. Such sites frequently depict mainstream media as presenting disinformation or fake news. Given this, a banner asking people to join the fight might even be conflated with a campaign by the disinformation website itself and would therefore not feel threatening to a reader. 


Sample screenshots of some of the ad banners. English translation, from left to right: (1) Don’t give disinformation a chance. Recognize Fake News. (2) Help to fight disinformation. Recognize Fake News. (3) Don’t give disinformation a chance. Fight Fake News!”
Website Design
The goal of www.echtwahr.info was to educate readers about fake news without alienating them or scaring them away. After clicking on a banner and being redirected to the website, the team’s goal was to get individuals to stay and learn by reading, taking quizzes, and honing their abilities to identify fake news. 

In creating a user-friendly website, our team limited the amount of pages and decided to trust scrolling as the preferred means of user interaction. In creating content based on research, the team worked to eliminate any unsubstantiated claims to ensure the website would be able to stand up to heightened scrutiny from German NGOs and journalists. Quiz topics, originally established in the audience mood boards, were also adjusted to a heightened degree of difficulty. “Fake news” items that were particularly hard to discern were selected and incorporated because our research had revealed that most fake news pieces were not obviously “fake” but instead did a very convincing job at imitating the language of actual news. 


Sample screenshot of www.echtwahr.info homepage.
Ad Buys
The project ran display, search, and video ads using Google Search360, DoubleClick (plus additional pieces) which broadcast campaign materials to at least three separate locations: Google Search results, banner ads on websites, and Youtube – as well as wherever else Google shows video ads. In total, $100,000 of ad buys were purchased to place banners in this manner. The ads primarily ran (with reporting) from May 20 through August 10, 2021. Nearly all of the ads ran in June and July, while a very small search campaign continued into September.


Sample screenshots of ad banners on target websites.


Sample screenshots of ad banners on target websites.

Ads on these platforms are purchased and placed on sites based on targeting criteria. Behavior, keywords, and topics can all be used to target users. Platforms like Google then match these topics and keywords to websites that make themselves available based on relevance to targeted criteria. Each of these overlapping layers of targeting criteria can be used (in tandem, individually, or some not at all) to reach desired audiences. Thus, items like keywords can be used to initially establish where ads may be placed, but user data and customized audiences also play a role in what is called “retargeting” (content which is shown to people over and over again). Notably, ads are not guaranteed to be shown. Rather, the targeted users must first visit the sites and then the ad must win out in a instantaneous auction-based competition against other ads vying to be shown in that space.

Accordingly, to decide what content to use to target our ads, we initially identified the top keywords within the Disinformation-Net through research and initial ad feedback from within the platform. Overall, three types of ads were involved:

Display Ads: appear on other websites with banner ad placements when people visit them. These ads primarily target people passively seeking information.
Search Ads: appear in the Google Search results at the top of the search results when people perform a search. These ads primarily target people actively seeking information.
Video Ads: appear on Youtube and other other places Google serves video ads. These ads simultaneously target people actively seeking information.

Google Search360 consolidates a number of separate Google Ad campaign types and interfaces into one platform to run ads simultaneously to all three. The platform targets users differently than in Google Ads, Google Display Ads, and Google Video Ads by allowing the advertiser to target using keywords, topics, and website placements all at the same time. The keyword targeting simultaneously applies to searches and websites, allowing advertisers to trigger on searches people make while also placing banner ads on websites and running video ads on videos that contain those keywords or relate to those topics. This type of keyword targeting can be narrowly or broadly defined.

Top keywords for impressions and clicks included:

Epochtimes
Fake news
Journalistenwatch
Sputnicknews
Rubikon

Top search terms included:


Sample screenshot of top search terms.
Findings
Overall Results
The data collected over the course of this research project confirmed our two hypotheses. Overall, our ads achieved 11 million impressions (almost entirely within the known sphere of disinformation spreaders that we targeted), confirming Hypothesis A. Hypothesis B was also confirmed, as users engaged with the targeted ad banners across both search and display ads. For those users who clicked on the video, we saw 1.6 million completed views (for an 81% video completion rate). Of the 28,000 clicks to the website, there were also 13,000 hours of viewed videos – establishing that people stayed on the site and watched the content. Interestingly, the gathered data suggested that clicks to the site originated from search and display ads, not video.

Top Keywords
For search ads targeted to specific user queries, the results of our top ten keywords and their associated impressions, clicks, and click-through rate (CTR) were as follows:



Top Websites
For display ads targeted to specific websites, the results of our top ten websites and their associated impressions, clicks, and CTR were as follows:



Of the many versions of the ads we ran, these four garnered the most clicks. 
Analysis
The data collected during this project revealed several key points related to search ads, display ads, and related CTR analysis. As noted, the data we collected generally indicated that clicks to the site were from search or display ads, not video.
Search Ads
The search ads were more successful at capturing users going to specific websites than the display ads were at capturing website visitors once they were on said websites.
In general, these ads had very high CTRs and succeeded in getting in front of searchers for the Epoch Times, Get Bad News (a site designed to stop people from engaging with disinformation), Uncut News (Swiss disinformation site), PolitikStube, Jouwatch, Reitschuster, Rubikon, and Sputnik News.

Display Ads
The display ads were successful at being placed on a range of questionable German news sites. 
Most of the top ten websites by clicks were also the top ten websites by impressions.

CTR Analysis
Comparing the display ads with CTR benchmarks is a bit difficult given that most benchmarks for display ads apply to commercial content (unlike the type of content being offered by our campaign). Depending on the source used, the average CTR benchmark is going to range from 0.35% (Hubspot, 2020) to 0.50% (Instapage, 2021).
Comparing the search ads with CTR benchmarks is also complicated for the same reason. Nonetheless, at the keyword level not only do the top ten keywords surpass the highest benchmark in a range, most of the top 25 keywords do as well. Depending on the source used, the average CTR benchmark is going to range from 1.91% (Hubspot, 2020) to 5.06% (Instapage, 2021).
Implications
Our overall findings reveal that we were able to reach at-risk consumers of disinformation and get them to engage with our targeted ads. As noted in the Literature Review, the ability to reach these users where they already are online is significant in the fight to counter the spread and impact of disinformation. Our findings are significant given the existing literature that has shown how content-neutral, accuracy-nudge interventions like these refocus user priorities on the value of accuracy, induce better sharing habits, and increase user ability to discern what is true and what is false. The benefit of effects like these, particularly when located within the disinformation sphere, cannot be understated. Visual representations of the impact and flow of the interventions are presented below.


The above graphic shows what a user might encounter on a typical disinformation website. The site often has disinformation-related advertising content as well.  

This graphic shows how a user would encounter a targeted display advertisement from this experiment. Not only does the intervention take the place of a potential secondary piece of disinformation, it also provides a nudge opportunity as well as an engagement opportunity in the form of a quiz. 

This graphic demonstrates how a user would encounter a targeted search advertisement from this experiment. In the above example, a user may be interrupted in their search that could otherwise potentially lead them to disinformation.


Although we were not able to measure and document whether the engagements in this project changed user attitudes and slowed the spread of mis- and disinformation, we are still able to draw related inferences based on the pilot project we conducted during the 2020 U.S. election. Results in that pilot included similar engagement rates, and therefore a corresponding positive effect could be assumed in Germany (though as recommended below, further studies would be required to verify this). Similarly, despite establishing that people did engage with this type of content, further testing will be necessary to establish the extent to which initial ad exposure and subsequent engagement ultimately affected user behavior. 

Critically, however, the baseline level of success in the German campaign was being able to find consumers of disinformation (based on our expert’s survey and analysis of the German disinformation sphere) and place our targeted, accuracy-nudge intervention ads in front of them. The fact that we are able to confirm Hypothesis A and achieve this means we were able to interrupt these users’ consumption of harmful content and prime them to think about whether or not what they were reading was indeed true. As established by prior research, simply putting the concept of fake news on such a user’s radar is a sufficient reminder to activate a state of higher discernment when consuming content – allowing them to act in line with their desire to share only accurate information. 

Given this, neither a CTR (as presented in the table above) or a cost-per-click / cost-per-engagement (which would be roughly $3.57 in this case) metric is a comprehensive way to capture the true impact and effect of the type of intervention employed in this study. Regardless of whether or not they clicked on an ad, users received an intervention nudging them towards accuracy based on the mere presence of the ad. Indeed, the tactic underlying this type of ad-based intervention is premised on research about exposure to the question (which does not have to include a subsequent click to make an impact, though further engagement could increase overall efficacy). This essential insight is why CTR is not the primary or best metric for encapsulating the overall efficacy of this study (even in situations such as search ads, which saw an extremely high CTR as compared to traditional advertising benchmarks). Nevertheless, we note the possibility that subsequent engagements and time spent on the www.echtwahr.info website could likely have an even greater impact on the user than just the initial ad exposure (due to the ability to learn more about disinformation and practice identifying fake news).
Limitations
Though this research builds off the pilot study initially conducted in the U.S., several limitations remain. The ads for this project were only run to target users in Germany, and were only displayed in German. These ads appeared only on certain, pre-selected websites and searches based on keyword targeting (and did not appear on social media). The primary limitation for this study relates to not tracking the downstream effects on users after viewing the ads or visiting www.echtwahr.info. Though we can infer that a limited exposure to a banner offering an accuracy-nudge intervention, or a more in-depth exposure through time spent on the website and quizzes, had a positive effect on users’ ability to discern true from false (and a correlating positive impact on sharing behavior), we cannot know the precise effect without experimenting further with the machine and using surveillance commerce to more precisely track and surveil user behavior.
Recommendations for Future Research
Measure Engagement Impact
Our findings demonstrate that surveillance advertising can be used to reach at-risk disinformation users and apply an accuracy-nudge intervention – priming them to more critically view the content they are consuming. As noted above, existing research on the benefits of priming for accuracy indicates that the type of interactions generated by the ad buys should have a positive impact on these users’ relationship with disinformation. Based on the data collected, we know users are indeed interacting with these interventions at times when they are most likely already consuming mis- or disinformation. Given this, the impact generated by these banners is likely great. More open research, however, still needs to be performed to confirm these findings and assess to what extent engaging with this type of content changes a reader’s relationship with disinformation. Indeed, taking these next steps is a significant reason why this initial study was conducted – what we hope will be the first in a series of studies examining the impact of targeted-advertising interventions on disinformation across global and issue-specific contexts (with each subsequent study building on the last).

Further studies should be conducted to measure whether this intervention (and any subsequent engagement on www.echtwahr.info) actually helped to stop the spread of misinformation and change user sharing decisions via assessment of measurable behaviors, such as reduced time spent on disinformation sites, reduced sharing, and a reduced likelihood of return. Campaign success could potentially be assessed by tracking user journeys through IPs, allowing for target group adjustments and website/banner content modifications to respond to interactions and increase efficacy. As demonstrated by the application of the pilot project’s procedures to the German campaign, the methods used in this study to identify at-risk disinformation users and produce relevant content should generally be replicable and transferable to any subsequent studies.

Notably, future studies like these would necessarily involve further, deeper engagement with the surveillance capitalism machine. Any such projects must therefore be fully in tune to the ethical considerations discussed in this paper. 
Add User Journey Granularity
Future research studies on this type of counteradvertising could also consider incorporating additional layers of granularity for user journeys to diversify a campaign based on where users were engaging with the ad banners and why. Based on our expert’s advice, we set out with a unified assumption that our target group would be predominantly comprised of consumers of fake news who were open to changing their mind – not people thoroughly committed to the conspiracy milieu. In turn, we shaped the content of our ads and website accordingly.

The study confirmed, however, that there are other kinds of users with different user journeys as well. Not all consumers of disinformation operate in the same way or with the same priorities – meaning they would likely respond to different messages in different ways. What might appeal to one user might be off-putting to another. A future project engaging on this topic could account for these differences by mapping out the various locations where consumers of disinformation could encounter ad banners, and building out an understanding of the different ways such consumers could be approached. To account for this variance, and to increase the likelihood of engagement and impact, different banners could be designed for different users. 

The following two tables illustrate potential disinformation-adjacent user types and ad pathways, along with the corresponding potential users journeys and ad engagement strategies. The options presented below represent only a few of the many existing possibilities – all of which could lead to varying or overlapping results. A future study could be made more effective by analyzing the types of ad engagement that appeal to different types of disinformation-prone user groups.

Table illustrating potential disinformation-adjacent user types and ad pathways to reach them. 



Table illustrating two potential user journeys using the above user types and ad engagement strategies. 

Although this level of granularity was not incorporated into the German project, it could be added (with sufficient time and resources) to future studies. Similar to the above suggestion to implement engagement impact measurements, the relative effectiveness of the different banners and placements could be measured through user journey IP tracking. Depending on performance, this method would allow for adjustments to be made based on target groups, banners, and website content. Any decision to increase granularity should be informed in advance by: (1) an awareness of the region/context in which the ads would be placed, (2) an understanding of the different user types to be targeted, (3) mapped out user journeys, and (4) the ability to adjust content based on subsequent changes in regional news and/or the disinformation landscape.
Policy Implications + Considerations
Several policy implications and considerations stem from this project’s research and findings, including:

Targeted public awareness campaigns, modeled after the research conducted in this project, should be employed by governments and foundations around the world as a new tool in their disinformation intervention toolboxes. 
Policymakers and companies should work to revamp the digital advertising ecosystem in a large-scale effort to disincentivize disinformation.
In evaluating the type of lasting impact created by educating individuals via targeted ad banner interventions, increased efforts should also be made to institutionalize digital literacy as a more comprehensive response to the threat posed by disinformation.

We discuss each of these three below.
Conduct Targeted Public Awareness Campaigns
The findings of this study demonstrate that targeted advertising can be a particularly effective route by which to engage with consumers of disinformation. As a result, public awareness campaigns utilizing targeted advertising on disinformation websites (or via social media platforms) in this manner would likely be effective from both a cost and impact perspective.

The damage inflicted on democracies by disinformation on social media, combined with the apparent effectiveness of targeted disinformation interventions, offers an ideal opportunity for government action. Public service announcements (PSAs) have long been utilized by governments to help inform members of the public on a wide range of issues. National awareness campaigns centered on societal issues are commonly employed throughout the world. Each year, hundreds of millions of dollars are spent in an effort to inform the public on issues such as emergency preparedness, hand-washing hygiene, or clean drinking water. These campaigns are often broadly targeted and attempt to reach wide swaths of the general public. 

Given the increased governmental interest in informing the public about digital issues and threats, and the significant threat posed by disinformation, the traditional, broad-based application of PSAs should be reconsidered. The hyper-targeted efforts carried out in this research project would be ideally suited to the PSA format – offering an efficient, content-neutral, and impactful way to inform members of the public who are at greatest risk of being influenced by disinformation. Through the use of such PSAs, governments would be able to speak to those that matter the most when it matters the most (i.e. when they are actively consuming disinformation). 

Notably, because the goal of such interventions would be to target a small subset of the overall population that is actively consuming disinformation content, the ability to “meet them where they are” would not require significant expense tied to the broad targeting processes of traditional PSAs. Furthermore, targeted ad banners like those used in this research project would likely result in a greater degree of effectiveness given that the members of the target audience often already have an inherent distrust of large media outlets (rendering a broadly targeted traditional PSA highly unlikely to work). And because these PSA ad banners would focus only on the subject of disinformation and whether readers could spot it, the substance of the PSAs would be able to remain content-neutral (i.e., apolitical).

Narrowly targeted public awareness campaigns would also cost far less than traditional PSAs. Due to their significant cost outlays, broadly focused public awareness campaigns routinely rely on in-kind donations from media outlets. As a result, radio and television stations typically donate substantial airtime for public service announcements (a trend that extends to digital platforms as well, which have donated ad impressions since at least 2010). Given the prevalence of donated advertising and technology companies public posturing around how it is in their interest to remove disinformation, any advertising undertaken in a targeted manner can and should be fully subsidized by the companies currently profiting off this problematic content. 

Indeed, donated advertising has become a primary mechanism by which social media platforms already support causes. So far, this type of work has typically involved public health issues such as COVID-19 (though disinformation has also been touched upon). In creating these messages, social media companies have undertaken various approaches including relying on existing content creators, purchasing national television advertisements, and funding media outlets. Yet efforts by policymakers to combat disinformation have been much more difficult to come by. To the extent they are funded, those that exist appear to be focused on foreign policy initiatives. This is particularly unfortunate because in contrast to these whole-of-society awareness campaigns, the type of intervention carried out in this research is focused on a narrow subset of society that can be reliably identified. 
Revamp Advertising Ecosystem to Disincentivize Disinformation 
The advertising industry plays a key role in allowing for the monetization of disinformation content – and therefore solutions designed to prevent the dissemination of such content must reflect this reality. The ability to profit off of purveying disinformation is dependent on advertisers paying for content to be displayed alongside it. At present, billions are made annually by the dominant platforms through disinformation content. While online advertising is purchased by scam artists and other malicious actors, those assisting platforms in monetizing this content also include a wide range of corporate giants, government agencies, and non-governmental organizations. One analysis found that for every $2.16 in digital ad revenue paid to legitimate newsbrands, advertisers sent $1 to misinformation websites. Nearly all such revenue comes from advertisers whose money plays a significant role in minimizing the financial incentives for social media platforms to stem the spread of disinformation. With the overwhelming majority of digital advertising being ‘programmatic’ or having the ads placed algorithmically, advertisers are frequently unaware as to where their content was specifically displayed. This lack of knowledge can be attributed in part to the ad-tech industry which is made up of numerous components – none of which maintain an obligation to ensure the advertisement is placed in a legitimate digital space. 

The extent to which this continues to be a problem is evidenced by the secondary industry which has cropped as a result. Companies in this adjacent space focus on ‘brand safety’ and ‘brand suitability,’ and range from corporations with diversified businesses like Oracle and ComScore to more focused ones like Peer39 and DoubleVerify. Despite their marketing materials, the industry has offered advertisers only rudimentary tools relying on keyword blocking and content classification. This has led to high-quality content outlets receiving less advertising revenue and conversely more for sites disseminating disinformation.  

In response, advertisers have created voluntary coalitions such as the Conscious Advertising Network, whose mission is to “ensure that industry ethics catches up with the technology of modern advertising,” and the Global Alliance for Responsible Media, a cross-industry initiative established by the World Federation of Advertisers to address the challenge of harmful content on digital media platforms and its monetization via advertising. Advertising trade associations have attempted to generate baseline standards as well.

While these efforts to increase voluntary compliance are no doubt meaningful, the extent of the problem necessitates greater scrutiny from policymakers. In turn, increasing attention is being paid to create and enforce a Code of Practice. These Codes have long been used to assist businesses in complying with government rules, historically focused on occupational health and safety. They are increasingly prevalent, however, in other industries such as food production, and in some instances have even been found to be admissible in court proceedings. In 2018, the European Commission led the way by establishing baseline standards to combat disinformation – gaining commitments from numerous large platforms and the advertising industry. The companies who participate, including Facebook, Twitter, Google, Microsoft, Mozilla and TikTok, submit reports on their efforts to monitor and combat disinformation. 

Since then, the European Commission has also released a report suggesting a range of improvements to the Code of Practice to better address the overall ecosystem underlying this challenge. Suggestions include evolving the Code of Practice into a Code of Conduct, expanding the signatories to ensure substantial representation by the advertising industry, establishing criteria for advertisement placement, creating a structured monitoring system based on performance indicators, enhancing demonetization efforts, encouraging the adoption of brand safety tools, and increasing fact-checking activities. While participation is currently voluntary, once the Digital Services Act comes into effect the landmark legislation is likely to introduce the ability to levy sanctions for those “that do not behave responsibly.”

The type of industries implicated in financially rewarding disinformation actors, abetting a thriving marketplace, and profiting massively, highlights the importance of oversight and enforcement – as well as the insufficiency of voluntary commitments. Despite their public statements to the contrary, large platforms continue to prioritize revenue generation above decisive action and succumb to the considerable incentives to allow disinformation to perpetuate. Until platforms drastically improve the identification of misinformation, expand fact-checking, and transition away from engagement-maximizing algorithms, formal regulatory authorities should compel compliance with best practices. A particular emphasis should be placed on designing tools that improve demonetization such as requiring standardized disclosure of advertising revenue and expenses based on known disinformation. Such a move would be likely to increase public pressure for these companies to improve and ideally redirect these ill-gotten profits towards the responsibly implemented rating efforts described above.
Institutionalize Digital Media Literacy
The research presented in this study (as well as that discussed in the Literature Review) sets the stage for evaluating the type of meaningful impact that can be made by educating individuals about the dangers of mis- and disinformation. Yet while targeted interventions through digital advertising were relied on in this instance (and are encouraged going forward), the extent to which disinformation is increasingly threatening society writ-large will ultimately necessitate a more comprehensive response. The ever-evolving and sophisticated dangers of going online, whether it be disinformation, malware, or scam artists, demonstrate the growing need for multi-layered solutions.

Though attention has long been paid to the need for “digital citizenship,” these efforts – which have ranged in topic and availability – have limited applicability to the unique threat posed by social media platforms. Fortunately, there is now an increasingly rich field of research and resources focused on ways to better inoculate people from the threats posed by disinformation. Thanks to heightened levels of funding from government allocations, corporate donations, and support from foundations, the effectiveness of the techniques underlying this approach have been repeatedly demonstrated and successful implementation stories abound. The EU and individual member states have supported numerous efforts for years, while the U.S. has recently increased their funding through the Infrastructure Investment and Jobs Act – which requires states to develop a Digital Equity Plan including digital literacy to be eligible for the associated federal grants.

A substantial subset of these efforts aim to assist specific segments of society, such as aging adults who are frequently cited as the most likely to share misinformation as well as believe it. Although prior spending and programs have focused on digital inclusion and helping older adults access the internet, these efforts have rarely included assistance with the need to determine the accuracy of information online. While this has changed somewhat in recent years with the creation of various resources and workshops, more needs to be done in order to systematically integrate these efforts into broader plans for the care of older adults. Another primary area of focus is the youngest segment of society where an opportunity exists to proactively prevent the current level of disinformation susceptibility from repeating itself in the future. The need for this type of work becomes more urgent as younger and younger children find their way online. Although there is a wide range of media literacy resources currently available to parents, teachers, and children, formal requirements should be incorporated into schools to ensure topics such as disinformation are covered. In this regard, the U.S. educational landscape is in a more nascent stage than the EU and other countries where efforts are already underway to generate resources to help teachers tackle disinformation in the classroom.￼ For more on surveillance capitalism, see The Age of Surveillance Capitalism. Available online at: https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism.￼ See, e.g., Lima, C. (2021, October 26). Facebook knew ads, microtargeting could be exploited by politicians. It accepted the risk. The Washington Post. Available online at: https://www.washingtonpost.com/politics/2021/10/26/facebook-knew-ads-microtargeting-could-be-exploited-by-politicians-it-accepted-risk/ ￼ See, e.g., Pennycook, G., Epstein, Z., Mosleh, M., et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 592, 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ See, e.g., Bank, M., Duffy, F., Leyendecker, V., & Silva M. (August 2021). The Lobby Network: Big Tech’s Web of Influence in the EU. Corporate Europe Observatory and LobbyControl e.V. Available online at: https://corporateeurope.org/sites/default/files/2021-08/The%20lobby%20network%20-%20Big%20Tech%27s%20web%20of%20influence%20in%20the%20EU.pdf.￼ See, e.g., The People’s Declaration. https://www.peoplesdeclaration.net/.￼ See, e.g., West, D. (2017, December 18). How to combat fake news and disinformation. Brookings. Available online at: https://www.brookings.edu/research/how-to-combat-fake-news-and-disinformation/.￼ Notably, although online disinformation spaces can advertise to increase their audience, this is not usually a primary route by which they expand. More often these spaces count on algorithmic referrals and holes in the data created by the current digital advertising framework. Attention metrics created by online advertising help drive disinformation due to the data advertisers create based on how they run their ads as well as the ad revenue potential for disinformation sites.￼ For the organizations involved in this project, tapping into a system that also commercializes consumer’s behavioral data was notably rife with ethical concerns (as detailed below).￼ See, e.g., Pennycook, G., Epstein, Z., Mosleh, M., et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 592, 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ Per an analysis by EUvsDisinfo, Germany is the EU state hit hardest by Russian false news. From 2015 to March 2021, the project counted 700 cases of disinformation targeting Germany, well ahead of France (300 cases) or Italy (almost 170 cases). Available online at: https://euvsdisinfo.eu/vilifying-germany-wooing-germany2/?highlight=germany.￼ See, e.g., Pennycook, G., Epstein, Z., Mosleh, M., et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 592, 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ Id.￼ Id. at 591.￼ Id. at 591-92.￼ Id. at 591.￼ Id. at 593.￼ Id.￼ Pennycook, G., McPhetres, J. Zhang, Y. et al. (2020). Fighting COVID-19 Misinformation on Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention. Psychological Science 770-780, Volume 31 Issue 7. Available online at: https://journals.sagepub.com/doi/pdf/10.1177/0956797620939054.￼ Id. at 770.￼ Id.￼ Id.￼ Id.￼ Id.￼ An overlapping team involving Pennycook and others has even created an “accuracy-prompt toolkit” designed to help to stem the spread of COVID-19 misinformation online. Epstein, Z., Berinsky, A., Cole, R. et al. (May 2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School Misinformation Review, Volume 2, Issue 3. Available online at: 
https://misinforeview.hks.harvard.edu/article/developing-an-accuracy-prompt-toolkit-to-reduce-covid-19-misinformation-online/.￼ Pennycook, G., Epstein, Z., Mosleh, M. et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 594, 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ Future studies could focus on the analysis of these measurable behaviors to better determine the impact of accuracy-nudge interventions in the wild. Limited studies have occurred on platforms such as Twitter. See, e.g., Pennycook, G., Epstein, Z., Mosleh, M. et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ See, e.g., Maksan, O. (2022, January 19). Treue Verbündete: Die AfD will sich in ihrer Nähe zu Russland von niemandem übertreffen lassen. Neue Zürcher Zeitung. Available online at: https://www.nzz.ch/international/treue-verbuendete-die-afd-will-sich-in-ihrer-naehe-zu-russland-von-niemandem-uebertreffen-lassen-ld.1665206?reduced=true; Baumholt, B. and Quast, P. (2022, February 27). AfD und Linke - Propagandahilfe für Putin. WDR. Available online at:
https://www1.wdr.de/nachrichten/landespolitik/Propagandahilfe-fuer-Russland-AfD-Linke-100.html
￼ Nocun, K. and Lamberty, P. (2020). Fake Facts: Wie Verschwörungstheorien unser Denken bestimmen. Quadriga; Kemper, A. (2020). Die Furchtbürger: Anmerkungen zum Rassismus, Sexismus und Klassismus der AfD. edition assemblage.￼ See, e.g., Steinmetz, K. (2018, August 9). How Your Brain Tricks You Into Believing Fake News. Time. Available online at: https://time.com/5362183/the-real-fake-news-crisis/.
￼ Hypothesis A: targeted advertising can be used to reach people who are consuming, or are about to consume, disinformation on websites.￼ Hypothesis B: such people are likely to engage with banner ads that ask them to identify disinformation.￼ Notably, four of these top sites were not strictly purveyors of disinformation per se, but could more accurately be described as yellow press (t-online.de; morgenpost.de; bild.de; and spiegel.de/politik).￼ See, e.g., Pennycook, G., Epstein, Z., Mosleh, M. et al. (2021). Shifting attention to accuracy can reduce misinformation online. Nature 592, 590–595. Available online at: https://www.nature.com/articles/s41586-021-03344-2.pdf.￼ As noted, the COVID-19 Pennycook research team has suggested a “suite” of interventions that social media platforms could easily apply. Pennycook, G., McPhetres, J. Zhang, Y. et al. (2020). Fighting COVID-19 Misinformation on Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention. Psychological Science 777, 770-780, Volume 31 Issue 7. Available online at: https://journals.sagepub.com/doi/pdf/10.1177/0956797620939054. An overlapping team involving Pennycook and others has even created an “accuracy-prompt toolkit” designed to help to stem the spread of COVID-19 misinformation online. Epstein, Z., Berinsky, A., Cole, R. et al. (May 2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School Misinformation Review, Volume 2, Issue 3. Available online at: 
https://misinforeview.hks.harvard.edu/article/developing-an-accuracy-prompt-toolkit-to-reduce-covid-19-misinformation-online/.￼ See, e.g., STOP.THINK.CONNECT.: Broad Government Industry and Non-Profit Coalition Unveils First-Ever Coordinated Online Safety Message. PR News Wire, October 4, 2010. Available online at: https://www.prnewswire.com/news-releases/stopthinkconnect-broad-government-industry-and-non-profit-coalition-unveils-first-ever-coordinated-online-safety-message-104282618.html.￼ See, e.g., #ThinkB4UClick (Think Before You Click). Available online at: https://defyhatenow.org/thinkb4uclick/.￼ See, e.g., Ghosh, D., Gorman, L., Schafer, B., & Tsao, C. (December 2020). The Weaponized Web: Tech Policy Through the Lens of National Security – Levers in the Digital Advertising Ecosystem. Alliance for Securing Democracy & Mossavar-Rahmani Center for Business and Government. Available online at: https://securingdemocracy.gmfus.org/wp-content/uploads/2020/12/The-Weaponized-Web.pdf; Melford, C. and Fagan, C. (May 2019). Cutting the Funding of Disinformation: The Ad-Tech Solution. Global Disinformation Index. Available online at: https://disinformationindex.org/wp-content/uploads/2019/05/GDI_Report_Screen_AW2.pdf; Sutcliffe, C. (October 1, 2021). ‘Disinformation is a business’: media execs explore how to demonetize falsehoods. The Drum. Available online at: https://www.thedrum.com/news/2021/10/01/disinformation-business-media-execs-explore-how-demonetize-falsehoods; Sanchez, S. (March 11, 2021). Brands spent at least $235 million on disinformation sites last year. PR Week. Available online at: https://www.prweek.com/article/1709700/brands-spent-least-235-million-disinformation-sites-last-year. ￼ Global Disinformation Index staff. (September 2019). The Quarter Billion Dollar Question: How is Disinformation Gaming Ad Tech? Global Disinformation Index. Available online at: https://disinformationindex.org/wp-content/uploads/2019/09/GDI_Ad-tech_Report_Screen_AW16.pdf; How Google Makes Millions Off of Fake News. Campaign For Accountability. Available online at: https://campaignforaccountability.org/work/how-google-makes-millions-off-of-fake-news/.￼ See, e.g., Jackson, J. (September 18, 2021). Financing Fake News: Nike and Amazon Advertise on Covid Conspiracy Sites. The Bureau of Investigative Journalism. Available online at: https://www.thebureauinvestigates.com/stories/2021-09-18/big-brands-advertising-fuelling-covid-misinformation-websites; Disinformation and Brand Safety also matters for NGOs. Global Disinformation Index, August 7, 2020. Available online at: https://disinformationindex.org/2020/08/disinformation-and-brand-safety-also-matters-for-ngos/.￼ Skibinski, M. Special Report: Top brands are sending $2.6 billion to misinformation websites each year. NewsGuard. Available online at: https://www.newsguardtech.com/special-reports/brands-send-billions-to-misinformation-websites-newsguard-comscore-report/.￼ See, e.g., Mandese, J. (February 24, 2020). IAB: Programmatic Now 85% Of All U.S. Digital Advertising, MediaPost. Available online at: https://www.mediapost.com/publications/article/347524/iab-programmatic-now-85-of-all-us-digital-adve.html.￼ Our Mission. Conscious Advertising Network. Available online at: https://www.consciousadnetwork.com/#Mission.￼ See, e.g., Code of Practice on Disinformation: Stemming the tide of fake news: Part II. Lexology, September 29, 2020. Available online at: https://www.lexology.com/library/detail.aspx?g=9f76bf1e-31fc-4842-b919-75b454a93e76. ￼ See, e.g., Codes of Practice. Safe Work Australia. Available online at: https://www.safeworkaustralia.gov.au/law-and-regulation/codes-practice.￼ See, e.g., Codes of Practice documents. Ministry for Primary Industries (New Zealand). Available online at: https://www.mpi.govt.nz/legal/compliance-requirements/codes-of-practice/; List of codes of practice. SafeWork NSW. Available online at: https://www.safework.nsw.gov.au/resource-library/list-of-all-codes-of-practice.￼ Code of Practice on Disinformation. European Commission. Available online at: https://digital-strategy.ec.europa.eu/en/policies/code-practice-disinformation.￼ Guidance on Strengthening the Code of Practice on Disinformation. European Commission, May 26, 2021. Available online at: https://digital-strategy.ec.europa.eu/en/library/guidance-strengthening-code-practice-disinformation.￼ Id.￼ EU Releases Guidance for Strengthening Code of Practice on Disinformation. Tech Policy Press, May 26, 2021. Available online at: https://techpolicy.press/eu-releases-guidance-for-strengthening-code-of-practice-on-disinformation/.￼ For an early example, see, e.g., Ribble, M., Bailey, G., & Ross, T. (September 11, 2004). Digital Citizenship: Addressing Appropriate Technology Behavior. Learning & Leading with Technology, v32. Available online at: https://eric.ed.gov/?id=EJ695788.￼ See, e.g., Digital learning and ICT in education. European Commission. Available online at: https://digital-strategy.ec.europa.eu/en/policies/digital-learning; FutureLabAE. European Association for the Education of Adults. Available online at: https://eaea.org/project/future-lab/?pid=11715.￼ H.R. 1841 - Digital Equity Act of 2021. 117th Congress (2021-2022). Available online at: https://www.congress.gov/bill/117th-congress/house-bill/1841/text.￼ Guess, A., Nagler, J., & Tucker, J. (January 9, 2019). Less than you think: Prevalence and predictors of fake news dissemination on Facebook. Science Advances. Available online at: https://www.science.org/doi/10.1126/sciadv.aau4586.￼ See, e.g., Feci, N. (September 8, 2020). Social Media Literacy: the new kid in class! KU Leuven CiTiP Blog. Available online at: https://www.law.kuleuven.be/citip/blog/social-media-literacy-the-new-kid-in-class-etdsml-project/; Digital Literacy in Education (Policy Brief). UNESCO Institute for Information Technologies in Education, May 2011. Available online at: https://iite.unesco.org/files/policy_briefs/pdf/en/digital_literacy.pdf.